{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUdiBRnF8Q91fr633Ouyuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duss02/LiveCodeBench-Lite-Evaluation/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiBMIw4qaFsQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "# Insert your Hugging Face token\n",
        "token = getpass.getpass(\"token hf \")\n",
        "login(token)"
      ],
      "metadata": {
        "id": "d9at19ika912"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# Change this to your model link\n",
        "model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "BDWNO5aQbJuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"livecodebench/code_generation_lite\", split=\"test\", streaming=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "csdUJiQjdlC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import time\n",
        "from io import StringIO\n",
        "import contextlib\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "MAX_SAMPLES_TO_EVALUATE = 100\n",
        "\n",
        "def format_prompt(prompt: str) -> str:\n",
        "    \"\"\"Formats the prompt for Qwen model\"\"\"\n",
        "    formatted = f\"<|im_start|>system\\nYou are a helpful coding assistant. Generate only Python code wrapped in ```python ... ``` unless otherwise specified.<|im_end|>\\n\"\n",
        "    formatted += f\"<|im_start|>user\\n{prompt}<|im_end|>\\n\"\n",
        "    formatted += f\"<|im_start|>assistant\\n\"\n",
        "    return formatted\n",
        "\n",
        "def generate_code(prompt: str, max_new_tokens: int = 400, temperature: float = 0.7, top_p: float = 0.95, timeout_seconds: int = 400) -> str:\n",
        "    \"\"\"Generates code using the loaded model with a timeout.\"\"\"\n",
        "    global model, tokenizer\n",
        "    full_prompt = format_prompt(prompt)\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n",
        "    generate_kwargs = {**inputs, \"max_new_tokens\": max_new_tokens, \"do_sample\": True, \"temperature\": temperature, \"top_p\": top_p, \"pad_token_id\": tokenizer.pad_token_id}\n",
        "    print(f\"Generating code for prompt: '{prompt[:50]}...'\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(**generate_kwargs, max_time=timeout_seconds)\n",
        "        end_time = time.time()\n",
        "        print(f\"Generation took {end_time - start_time:.2f} seconds.\")\n",
        "        generated_token_ids = output_ids[0][inputs['input_ids'].shape[1]:]\n",
        "        generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
        "        generated_text = generated_text.replace(\"<|eot_id|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
        "        print(f\"Raw generated text:\\n---\\n{generated_text}\\n---\")\n",
        "        return generated_text\n",
        "    except RuntimeError as e:\n",
        "        if \"Time limit exceeded\" in str(e):\n",
        "            raise TimeoutError(f\"Code generation exceeded the time limit of {timeout_seconds} seconds.\")\n",
        "        else:\n",
        "            raise  # Rilancia altre RuntimeErrors\n",
        "    except TimeoutError:\n",
        "        raise\n",
        "\n",
        "def extract_python_code(generated_text: str) -> str | None:\n",
        "    \"\"\"Extracts Python code block, with fallback.\"\"\"\n",
        "    code_block_match = re.search(r\"```python\\n(.*?)\\n```\", generated_text, re.DOTALL)\n",
        "    if code_block_match: return code_block_match.group(1).strip()\n",
        "    else:\n",
        "        print(\"Warning: Could not find Python code block ```python ... ```. Assuming entire output is code.\")\n",
        "        if re.search(r\"^\\s*(def |class |import |from )\", generated_text, re.MULTILINE): return generated_text.strip()\n",
        "        else: print(\"Warning: Fallback code doesn't look like Python definition/import. Skipping execution.\"); return None\n",
        "\n",
        "# --- valuta ---\n",
        "def evaluate_solution(generated_code: str, test_code: str) -> tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Executes the generated code and runs the test code against it.\n",
        "    Prevents blocking hangs by replacing sys.stdin to cause EOFError on input().\n",
        "    \"\"\"\n",
        "    execution_namespace = {}\n",
        "    stdout_capture = StringIO()\n",
        "    stderr_capture = StringIO()\n",
        "    original_stdin = sys.stdin\n",
        "\n",
        "    try:\n",
        "        # Questo farà sì che input() sollevi EOFError invece di bloccarsi\n",
        "        print(\"Replacing sys.stdin with dummy stream to prevent blocking input() calls.\")\n",
        "        sys.stdin = StringIO('')\n",
        "\n",
        "        print(\"Executing generated code...\")\n",
        "        #  redirect_stdout/stderr per catturare l'output\n",
        "        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):\n",
        "            exec(generated_code, execution_namespace)\n",
        "\n",
        "        print(\"Executing test code...\")\n",
        "        with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):\n",
        "            exec(test_code, execution_namespace)\n",
        "\n",
        "        print(\"Tests Passed.\")\n",
        "        return True, f\"Passed.\\nStdout:\\n{stdout_capture.getvalue()}\\nStderr:\\n{stderr_capture.getvalue()}\"\n",
        "\n",
        "    except EOFError:\n",
        "        print(\"Evaluation Failed: Code attempted to read from stdin (e.g., using input()).\")\n",
        "        tb_str = traceback.format_exc()\n",
        "        return False, f\"Failed: Code required user input (EOFError on reading stdin).\\nStderr:\\n{stderr_capture.getvalue()}\\nTraceback:\\n{tb_str}\"\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"Test Failed (AssertionError): {e}\")\n",
        "        tb_str = traceback.format_exc()\n",
        "        return False, f\"Failed: AssertionError: {e}\\nStderr:\\n{stderr_capture.getvalue()}\\nTraceback:\\n{tb_str}\"\n",
        "    except SyntaxError as e:\n",
        "        print(f\"Syntax Error in generated code: {e}\")\n",
        "        tb_str = traceback.format_exc()\n",
        "        return False, f\"Failed: SyntaxError: {e}\\nStderr:\\n{stderr_capture.getvalue()}\\nTraceback:\\n{tb_str}\"\n",
        "    except Exception as e:\n",
        "        # Gestisce tutti gli altri errori di esecuzione\n",
        "        print(f\"Execution Error: {e} (Type: {type(e).__name__})\")\n",
        "        tb_str = traceback.format_exc()\n",
        "        # print(f\"Full Traceback:\\n{tb_str}\")\n",
        "        return False, f\"Failed: Execution Error: {e}\\nType: {type(e).__name__}\\nStderr:\\n{stderr_capture.getvalue()}\\nTraceback:\\n{tb_str}\"\n",
        "    finally:\n",
        "        print(\"Restoring original sys.stdin.\")\n",
        "        sys.stdin = original_stdin\n",
        "\n",
        "# --- Main ---\n",
        "results = {}\n",
        "passed_count = 0\n",
        "attempted_count = 0\n",
        "\n",
        "print(\"\\n--- Starting Evaluation from LiveCodeBench Stream ---\")\n",
        "print(f\"Will evaluate up to {MAX_SAMPLES_TO_EVALUATE} samples.\")\n",
        "\n",
        "try:\n",
        "    for i, sample in enumerate(dataset):\n",
        "        if attempted_count >= MAX_SAMPLES_TO_EVALUATE:\n",
        "            print(f\"\\nReached evaluation limit of {MAX_SAMPLES_TO_EVALUATE} samples.\")\n",
        "            break\n",
        "\n",
        "        attempted_count += 1\n",
        "        sample_id = sample.get(\"problem_id\", f\"streamed_sample_{i}\")\n",
        "        print(f\"\\n--- Evaluating Sample {attempted_count}/{MAX_SAMPLES_TO_EVALUATE}: {sample_id} ---\")\n",
        "\n",
        "        difficulty = sample.get(\"difficulty\", \"Unknown\")\n",
        "        print(f\"Problem Difficulty: {difficulty}\")\n",
        "\n",
        "        results[sample_id] = {\"passed\": False, \"message\": \"Evaluation not started\", \"generated_code\": None, \"difficulty\": difficulty}\n",
        "\n",
        "        try:\n",
        "            problem_prompt = sample[\"question_content\"]\n",
        "            test_code = sample[\"public_test_cases\"]\n",
        "\n",
        "            raw_generated_text = generate_code(problem_prompt)\n",
        "            extracted_code = extract_python_code(raw_generated_text)\n",
        "\n",
        "            passed = False\n",
        "            message = \"Failed: No valid Python code extracted.\"\n",
        "            if extracted_code:\n",
        "                passed, message = evaluate_solution(extracted_code, test_code)\n",
        "            else:\n",
        "                print(message)\n",
        "\n",
        "            results[sample_id][\"passed\"] = passed\n",
        "            results[sample_id][\"message\"] = message\n",
        "            results[sample_id][\"generated_code\"] = extracted_code or raw_generated_text\n",
        "\n",
        "            if passed:\n",
        "                passed_count += 1\n",
        "            print(f\"Result for {sample_id}: {'PASSED' if passed else 'FAILED'}\")\n",
        "\n",
        "        except TimeoutError as e:\n",
        "            error_message = f\"Failed: Code generation timed out after 4 minutes.\"\n",
        "            print(f\"Error: {error_message}\")\n",
        "            results[sample_id][\"message\"] = error_message\n",
        "        except KeyError as e:\n",
        "            error_message = f\"Failed: Missing key in dataset sample: {e}. Check dataset structure.\"\n",
        "            print(f\"Error: Skipping sample {sample_id} due to missing key: {e}. Check dataset structure.\")\n",
        "            results[sample_id][\"message\"] = error_message\n",
        "        except Exception as e:\n",
        "            error_message = f\"Failed: Unexpected error during processing: {e}\"\n",
        "            print(f\"Error: Skipping sample {sample_id} due to unexpected error during processing: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "            results[sample_id][\"message\"] = error_message\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nError: Variable not defined (likely 'model', 'tokenizer', or 'dataset'): {e}\")\n",
        "    print(\"Please ensure 'model', 'tokenizer', and 'dataset' are loaded before running the evaluation loop.\")\n",
        "    print(traceback.format_exc())\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading or iterating through the dataset: {e}\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "if attempted_count > 0:\n",
        "    pass_at_1 = passed_count / attempted_count\n",
        "    print(f\"\\nFinal Pass@1 Score: {pass_at_1:.4f} ({passed_count}/{attempted_count} passed out of {attempted_count} attempted samples)\")\n",
        "\n",
        "    difficulty_results = {}\n",
        "    for res in results.values():\n",
        "        diff = res.get(\"difficulty\", \"Unknown\")\n",
        "        if diff not in difficulty_results: difficulty_results[diff] = {\"passed\": 0, \"attempted\": 0}\n",
        "        difficulty_results[diff][\"attempted\"] += 1\n",
        "        if res[\"passed\"]: difficulty_results[diff][\"passed\"] += 1\n",
        "\n",
        "    print(\"\\nPass Rate per Difficulty:\")\n",
        "    for diff, counts in difficulty_results.items():\n",
        "        if counts[\"attempted\"] > 0:\n",
        "            pass_rate = counts[\"passed\"] / counts[\"attempted\"]\n",
        "            print(f\"- {diff}: {pass_rate:.4f} ({counts['passed']}/{counts['attempted']})\")\n",
        "        else:\n",
        "            print(f\"- {diff}: No samples attempted\")\n",
        "else:\n",
        "    print(\"\\nNo samples were attempted evaluation (check dataset loading or iteration).\")\n",
        "\n",
        "\n",
        "import json\n",
        "with open(\"result_eval_100.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)"
      ],
      "metadata": {
        "id": "5bw9zCe7wdsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
